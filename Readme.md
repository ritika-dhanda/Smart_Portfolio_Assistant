# ðŸ¤– Smart Portfolio Interview Assistant

**Smart Portfolio Interview Assistant** is an AI-powered resume chatbot that reads a candidate's resume, answers questions about skills and projects, and can run a *Mock Interview Mode* to help job seekers practice for technical interviews.

Built with **FastAPI** (backend), **Sentence-Transformers** (embeddings), **Groq LLM** (Llama 3.3 model), and **React + Vite** (frontend).

## Features

- âœ… Upload a PDF resume and extract text automatically.
- âœ… Create and store resume embeddings for fast retrieval.
- âœ… Ask natural language questions about your resume (skills, projects, experience).
- âœ… **Mock Interview Mode** â€” the assistant asks interview-style questions based on your resume and gives feedback.
- âœ… Conversation memory for contextual follow-ups.
- âœ… Markdown-formatted responses for clear, readable output.
- âœ… Modern, polished UI suitable for portfolio demos.

---

## Tech Stack

- **Frontend:** React, Vite, react-markdown, remark-gfm  
- **Backend:** FastAPI, Uvicorn  
- **Embeddings:** `sentence-transformers` (all-MiniLM-L6-v2)  
- **LLM:** Groq (Llama 3.3 via Groq API)  
- **Storage:** Local filesystem for resume embeddings (can be swapped for DB/IPFS)

---

## Project Structure

```
smart_portfolio/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ data/                         # resume_embeddings.pkl (generated)
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ chatbot.py
â”‚   â”‚   â””â”€â”€ upload_resume.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ .env.example
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ Chatbot.jsx
â”‚   â”‚   â”‚   â””â”€â”€ ResumeUpload.jsx
â”‚   â”‚   â”œâ”€â”€ App.jsx
â”‚   â”‚   â”œâ”€â”€ main.jsx
â”‚   â”‚   â””â”€â”€ index.css
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vite.config.js
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## Quickstart (Local)

> Make sure you have Python 3.10+ and Node.js installed.

### Backend

1. Create and activate a virtual environment (recommended inside backend folder):
```bash
cd backend
python -m venv .venv
# Windows (PowerShell)
.venv\Scripts\Activate.ps1
# macOS / Linux
source .venv/bin/activate
```

2. Install backend deps:
```bash
pip install -r requirements.txt
```

3. Create `.env` (copy from `.env.example`) and set:
```
GROQ_API_KEY=your_groq_api_key_here
MODEL_NAME=llama-3.3-70b-versatile
FRONTEND_URL=http://localhost:5173
```

4. Run backend:
```bash
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

Backend Swagger: `http://127.0.0.1:8000/docs`

---

### Frontend

1. In a new terminal:
```bash
cd frontend
npm install
```

2. Start dev server:
```bash
npm run dev
```

Open: `http://localhost:5173`

**Flow:** Upload resume on Upload page â†’ embeddings are created â†’ switch to Chat page â†’ ask questions or start Mock Interview.

---

## Mock Interview Mode (how it works)

- The assistant analyzes your resume for technical topics and picks a set of interview-style questions (behavioral + technical).
- Mode behaviour (from the UI): send message `Start mock interview` or press the â€œStart mock interviewâ€ button (if included). The assistant:
  1. Asks 4â€“8 questions tailored to resume items.
  2. Waits for your answer (you type).
  3. Gives feedback: correctness, tips, suggested talking points, and sample improved answers.
- Use for practice and to generate bullet points for talking about projects.

---

## API Endpoints (important)

- `POST /resume/upload` â€” Upload resume PDF (`file` form field). Generates `backend/data/resume_embeddings.pkl`.
- `POST /chatbot/ask` â€” Body: `{ "message": "..." }`. Returns `{ "response": "..." }`.

---

## Deployment (short summary)

1. **Backend (Render)**: create a Web Service pointing to `backend/` â€” `pip install -r requirements.txt` and start `uvicorn main:app --host 0.0.0.0 --port 10000`. Add env vars in Render (GROQ_API_KEY, MODEL_NAME, FRONTEND_URL).
2. **Frontend (Vercel)**: point to `frontend/`, build command `npm run build`, output `dist`. Add `VITE_API_BASE` env var pointing to Render backend URL.

See the **Deployment** section later in this README for step-by-step instructions.

---

## Environment Variables

**Backend `.env`**
```
GROQ_API_KEY=...
MODEL_NAME=llama-3.3-70b-versatile
FRONTEND_URL=http://localhost:5173
```

**Frontend `.env`** (optional, used if you reference `VITE_API_BASE`):
```
VITE_API_BASE=http://localhost:8000
```

> Never commit real API keys â€” add `.env` to `.gitignore`.

---

## Development Tips

- If responses seem generic, re-upload the resume to refresh embeddings.
- For reproducible environments, create `requirements.txt` from your virtualenv via `pip freeze > requirements.txt`.
- To debug CORS issues: ensure `FRONTEND_URL` is in backend CORS allow list or set `allow_origins=["*"]` while developing.


## Screenshots

![alt text](image-1.png)
![alt text](image.png)

## Live Demo

## License

MIT Â© Ritika Dhanda

---

## Contact

Ritika Dhanda â€” RITIKA_21BCS8217 â€” [LinkedIn](#) â€¢ [GitHub](#)
